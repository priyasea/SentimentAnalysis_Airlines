{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30170e5c-4592-4fd8-a7be-15e39834afb9",
   "metadata": {},
   "source": [
    "# Airline Tweets Sentiment Analysis using Machine Learning  \n",
    "### Predicting the sentiment as Positive/Negative with LinearSVC & FastAPI | Dockerized & Deployed on Render"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcefc71-a073-4a57-a996-fe9e72dabcc2",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "When people complete their journeys on airplanes, they tweet about it upon landing or reaching their home. These tweets can tell about a positive experience or negative experience with the flight carrier. The Airlines operating the flight have to scan through these tweets and determine which are the negative ones and respond to them accordingly assuring of better service next time or any amendments. \n",
    "This machine learning project will help Airlines to identify negative reviews and take appropriate steps for improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e199ea98-d9b8-48e3-8c66-d190ef6ee778",
   "metadata": {},
   "source": [
    "The objective of this project is to develop a machine learning–based Airline Tweets Sentiment Analysis Prediction system that predicts whether the tweet is positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e740c1f-4e0d-4ade-b0aa-32120b05964d",
   "metadata": {},
   "source": [
    "The project includes:\n",
    "\n",
    "- Data cleaning and preprocessing\n",
    "- Exploratory Data Analysis (EDA)  \n",
    "- Feature engineering and selection\n",
    "- Model training and comparison (Logistic Regression, LinearSVC, Complimentary Naive Bayes)  \n",
    "- Hyperparameter tuning\n",
    "- Training final model\n",
    "- Deploying the model using FastAPI and Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3091c-5956-4bd0-9a55-bb0a93dcf906",
   "metadata": {},
   "source": [
    "The final solution is built using the LinearSVC model as we got best F! score with this model\n",
    "\n",
    "The model is deployed as a REST API using FastAPI, containerized using Docker, and hosted on Render for real-time inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2aaeda-5446-4e9f-9544-b20d112eae3e",
   "metadata": {},
   "source": [
    "## How the Solution Is Used\n",
    "\n",
    "### 1. Transaction Prediction \n",
    "\n",
    "When a transaction is processed, its details are sent to the `/predict` endpoint. The API responds with sentiment and classification. \n",
    "\n",
    "#### Example request:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "  \"text\": \"It was not at all a good flight. We were stranded at Frankfurt which was a stopover\",\n",
    "  \"airline\": \"american\",\n",
    "  \"retweet_count\": 0\n",
    "}\n",
    "\n",
    "]\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e310173-a692-4441-b1d8-ef33c45d7c2c",
   "metadata": {},
   "source": [
    "#### Example Response\n",
    "```json\n",
    "\n",
    "{\n",
    "  \"sentiment\": 0,\n",
    "  \"review\": \"Negative Review\"\n",
    "}\n",
    "\n",
    "```\n",
    "Based on the model response:\n",
    "\n",
    "| Prediction                      | Recommended Action       |\n",
    "|---------------------------------|--------------------------|\n",
    "| Negative Review                 | Take action for improvement/apology\n",
    "| Positive Review                 | Thank customer                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80809ba2-c174-4106-920c-b2981f9899bb",
   "metadata": {},
   "source": [
    "### Summary of integration\n",
    "\n",
    "- Supports real-time sentiment prediction\n",
    "- Returns a probability 1/0 and review prediction\n",
    "- Suitable for integration with Twitter Corpus of airline reviews\n",
    "- Helps airlines improve their services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca12a4-4a7f-4b23-8f4f-55e966c5b54c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e93cab80-1330-4228-9972-a40c1bd3c92e",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 1. Dataset Overview\n",
    "\n",
    "- **Total transactions:** `4008`\n",
    "- **Positive Reviews:** `1781` (~44.43%)\n",
    "- **Negative Reviews:** `2227` (~55.56%)\n",
    "- **Missing values:** `user_timezone : 1280`\n",
    "\n",
    "**Data source:** [Airline Sentiment Prediction Dataset(Kaggle)]  \n",
    "(https://www.kaggle.com/competitions/bootcamp-the-basics-of-mla/data)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Feature Overview\n",
    "\n",
    "| Feature Type | Features |\n",
    "|--------------|----------|\n",
    "| **Identifier** | `Id`|\n",
    "| **Numerical** | `retweet_count` |\n",
    "| **Non Numeric** | `airline`, `text`, `user_timezone` |\n",
    "| **Target variable** | `airline_sentiment` (positive = Positive Review ,negative = Negative Review) |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Class Distribution (Class Imbalance)\n",
    "\n",
    "The target variable is slightly imbalanced:\n",
    "- **Positive Reviews:** `1781`\n",
    "- **Negative Reviews:** `2227`\n",
    "\n",
    "**Image in repository at:** `images/class_distribution.png`\n",
    "\n",
    "![Sentiment Prediction Counts](images/class_distribution.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7671a5bb-810a-47e4-b13b-cb3ddb9aa7e0",
   "metadata": {},
   "source": [
    "\n",
    "**Key insight:**\n",
    "- Accuracy is not sufficient to judge the models → we focused on **F1-score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93684fb-cd17-4bd9-b93c-e22de7a4871a",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We encoded the airline_sentiment feature which was a categorical feature into numeric feature called sentiment as follows<br>\n",
    "`df[\"sentiment\"] = (df[\"airline_sentiment\"] == \"positive\").astype(int)`\n",
    "\n",
    "It was necessary to encode most important feature which is **text** certain into numeric to capture general characteristics of negative tweets . \n",
    "Some features were dropped as they were not influencing the Sentiment Variable.\n",
    "Feature enginering has been done for EDA within the notebook. <br>\n",
    "The `FeatureEngineering` is implemented  **src/feature_engineering.py**. This transformation is applied as the first step of the ML pipeline \n",
    "to ensure consistency during both training and inference.\n",
    "\n",
    "### Key Transformations\n",
    "\n",
    "| Feature | Description | Motivation |\n",
    "|--------|-------------|------------|\n",
    "| `airline_sentiment`|`sentiment` | Airline Sentiment is encoded into binary numeric feature 0/1 |\n",
    "| `text` |`text_length`| text_length is a numeric feature derived from text which is the number of characters in tweet. |\n",
    "| `text` | `word_count` |word_count is a numeric feature derived from text which is the number of words in tweet. |\n",
    "| `text` | `neg_word_count` |neg_word_count is a numeric feature derived from text which is the number of negative words in tweet. |\n",
    "| `text` | `all_caps_count` |all_caps_count is a numeric feature derived from text which is the number of words in CAPS in tweet. |\n",
    "| `text` | `exclamations` |exclamations is a numeric feature derived from text which is the number of exclamations in tweet. |\n",
    "| `text` | `has_negation` |has_negation is a numeric(binary) feature derived from text which indicates presence of negation like no/not/never etc. |\n",
    "| **Dropped:** `Id`| Removed unique identifiers | Prevent data leakage and overfitting |\n",
    "| **Dropped:** `user_timezone` | Has negligible impact on target variable | Has many missing values |\n",
    "| **Dropped:** `airline` | May have slight impact on target variable| removed this feature to not introduce bias for an airline|\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- Most machine-learning algorithms operate on numbers, not labels or strings.\n",
    "- We used Text feature to derive several numeric features like text_length, neg_word_count so that model learns characteristic features of negative and positive tweets\n",
    "- Prevents **data leakage** by excluding unique identifiers\n",
    "- Dropped columns like airline and user_timezone so that model can be built on important features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a344bb41-9feb-4552-a890-5db550a878c3",
   "metadata": {},
   "source": [
    "### EDA after Feature Engineering \n",
    "\n",
    "Due to limited features, we did EDA after feature engineering as follows:-\n",
    "\n",
    "\n",
    "#### Average Text Count by Sentiment\n",
    "**Image in repository at:** `images/Average Text Length (characters) by Sentiment.png`\n",
    "\n",
    "![Sentiment Prediction Counts](images/Average Text Length (characters) by Sentiment.png)\n",
    "\n",
    "#### Average Word Count by Sentiment\n",
    "**Image in repository at:** `images/Average Text Length (characters) by Sentiment.png`\n",
    "\n",
    "![Sentiment Prediction Counts](images/Average Text Length (characters) by Sentiment.png)\n",
    "\n",
    "#### Average Exclamation Count by Sentiment\n",
    "**Image in repository at:** `images/Average Text Length (characters) by Sentiment.png`\n",
    "\n",
    "![Sentiment Prediction Counts](images/Average Text Length (characters) by Sentiment.png)\n",
    "\n",
    "#### Average Negative Word Count by Sentiment\n",
    "**Image in repository at:** `images/Average Text Length (characters) by Sentiment.png`\n",
    "\n",
    "![Sentiment Prediction Counts](images/Average Text Length (characters) by Sentiment.png)\n",
    "\n",
    "#### Average Retweet Count by Sentiment\n",
    "**Image in repository at:** `images/Average Text Length (characters) by Sentiment.png`\n",
    "\n",
    "![Sentiment Prediction Counts](images/Average Text Length (characters) by Sentiment.png)\n",
    "\n",
    "#### Average All CAPS Count by Sentiment\n",
    "**Image in repository at:** `images/Average Text Length (characters) by Sentiment.png`\n",
    "\n",
    "![Sentiment Prediction Counts](images/Average Text Length (characters) by Sentiment.png)\n",
    "\n",
    "\n",
    "#### Text Distribution \n",
    "\n",
    "\n",
    "#### Word Count Distributuon \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9fe433-ea5f-478d-b52d-22279eceb944",
   "metadata": {},
   "source": [
    "### 4. Mutual Information (Categorical/Binary Features)\n",
    "\n",
    "| Feature | MI Score |\n",
    "|---------|----------|\n",
    "| `employment_status` | 0.175941 |\n",
    "| `grade_subgrade` | 0.026769 |\n",
    "| `loan_purpose` | 0.000325 |\n",
    "| `education_level` | 0.0048 |\n",
    "| `gender ` | 0.000028 |\n",
    "| `marital_status ` | 0.000003 |\n",
    "\n",
    "\n",
    "**Insight:**\n",
    "- Employment Status is a strong feature in dataset. It indicates whether customer will pay back the loan. Grade Subgrade is also a good feature\n",
    "- Some features like loan_purpose and education_level have less influence, but can prove to be useful after encoding\n",
    "- gender and marital status dont have much influence on loan payback intention hence these will be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5524630-1044-46e9-af37-21069594a71f",
   "metadata": {},
   "source": [
    "### 5. Correlation of Numeric Features\n",
    "\n",
    "**Image in repository:** `images/correlation_matrix.png`\n",
    "\n",
    "![Correlation Matrix of Numeric Features](images/correlation_matrix.png)\n",
    "\n",
    "**Important correlations with `is_fraud`:**\n",
    "- `debt_to_income_ratio` → 0.335758\n",
    "- `credit_score` → 0.234319\n",
    "- `interest_rate` → -0.130789\n",
    "\n",
    "#### Interpretation:\n",
    "* **High Credit Score** → Loan Paid Pack probability is higher \n",
    "* **Low Debt to Income ratio** → Loan Paid Pack probability is higher \n",
    "* **Lower interest rates** →Loan Paid Pack probability is higher "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3be11-3832-402b-bf31-e0cf85b125c6",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "It was necessary to encode certain categorical features into numeric to improve model performance. Some features were dropped as they were not influencing the Loan Pay Back Variable.\n",
    "The `FeatureEngineering` is implemented  **src/features.py**. This transformation is applied as the first step of the ML pipeline to ensure consistency during both training and inference.\n",
    "\n",
    "### Key Transformations\n",
    "\n",
    "| Feature | Description | Motivation |\n",
    "|--------|-------------|------------|\n",
    "| `education_level`|`education_encoded` | Education Level is encoded using the Ordinal encoder |\n",
    "| `grade_subgrade` |`grade_code`| Grade Subgrade is encoded using the Ordinal Encoder |\n",
    "| `loan_purpose` | loan_purpose_te |Loan Purpose is encoded using target encoder|\n",
    "| **Dropped:** `id`| Removed unique identifiers | Prevent data leakage and overfitting |\n",
    "| **Dropped:** `education_level`, `grade_subgrade`, `loan_purpose` | Replaced by encoded features | Avoid redundancy |\n",
    "| **Dropped:** `marital_status`, `gender` | Has negligible impact on target variable loan_paid_back | feature reduction |\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- Most machine-learning algorithms operate on numbers, not labels or strings.\n",
    "- By converting education_level using ordinal encoder , it helps the model capture the inherent distance between various education levels\n",
    "- By converting grade_subgrade using ordinal encoder, it helps model capture the distance between various grades.\n",
    "- Loan Purpose was converted using target encoding because some loans like education and medical loan and riskier to give.\n",
    "- Prevents **data leakage** by excluding unique identifiers\n",
    "- Dropped columns like gender and marital status so that model can be built on important features.\n",
    "\n",
    "### Pipeline Integration\n",
    "\n",
    "The transformer is used as part of the final ML pipeline:\n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"featureengineering\", FeatureEngineering()),\n",
    "        (\"vectorizer\", DictVectorizer(sparse=False)),\n",
    "        (\"model\", final_model),  # XGBClassifier\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08acb7af-3292-4ac5-828d-f7071ca7a6e4",
   "metadata": {},
   "source": [
    "## Model Training & Selection\n",
    "\n",
    "The dataset was split into:\n",
    "* 60% Training\n",
    "* 20% Validation\n",
    "* 20% Testing\n",
    "\n",
    "Multiple models were trained using the training set and evaluated against the validation set. Hyperparameter tuning and threshold optimization were performed to maximize predictive performance, especially focusing on F1-score and Recall, which are critical for fraud detection.\n",
    "\n",
    "### Models Evaluated\n",
    "\n",
    "| Model | Tuned Parameters | Decision Threshold | ROC-AUC | Accuracy | F1 Score |\n",
    "|-------|------------------|-------------------|---------|-----------|----------|\n",
    "| Logistic Regression | `solver = lbfgs, C=1, max_iter=1000` | 0.4 | 0.909741 | 0.9016 | 0.941174 | \n",
    "| Decision Tree | `max_depth=10, min_samples_leaf=100, random_state=42` | 0.4 | 0.91415   | 0.901918  | 0.941701  |\n",
    "| Random Forest | `max_depth=10, min_samples_leaf=20, n_estimators=300, n_jobs=-1, max_features=sqrt` | 0.55 | 0.912805 | 0.902701 | 0.941708 |\n",
    "| XGBoost (Final) | `objective='binary:logistic', eval_metric='auc', subsample=1.0, n_estimators=450, min_child_weight=30, max_depth=6, learning_rate=0.1, n_jobs=-1, random_state=42` | 0.45 | 0.921519 | 0.904688 | 0.943001 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e688e2-b1ae-41bf-bb7b-644ec75ff735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b981f50b-ec00-48e9-9ac9-c7adf087d387",
   "metadata": {},
   "source": [
    "### Final Model Selection\n",
    "\n",
    "After comparing performance across models, **XGBClassifier** was selected as the final production model based on the following:\n",
    "\n",
    "* Highest ROC-AUC on validation  \n",
    "* Best F1-score, indicating strong Loan Payback probability calculation \n",
    "\n",
    "### Final Model Evaluation (on Test Set)\n",
    "\n",
    "After selecting XGBClassifier, the model was retrained using full train + testing datasets, and final evaluation was performed on the test set.\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| ROC-AUC | 0.9781 |\n",
    "| F1 Score | 0.8025 |\n",
    "| Decision Threshold | 0.80 |\n",
    "\n",
    "## Exporting Notebook to Script\n",
    "\n",
    "To comply with project requirements and ensure reproducibility, all essential machine learning steps developed in the notebook (`notebooks/notebook.ipynb`) were fully converted into Python scripts.\n",
    "\n",
    "### Scripts Created\n",
    "\n",
    "| Script | Purpose |\n",
    "|--------|---------|\n",
    "| `src/train.py` | Contains final model training pipeline and saves the trained model. |\n",
    "| `src/predict.py` | Loads the trained model and serves predictions via a FastAPI REST endpoint. |\n",
    "| `src/featureproc.py` | Implements the custom feature engineering logic. |\n",
    "\n",
    "### What Was Exported from Notebook\n",
    "The following core logic developed and validated in `notebooks/notebook.ipynb` was migrated into standalone scripts for production readiness:\n",
    "\n",
    "| Exported Component | Implemented In | Description |\n",
    "|-------------------|----------------|-------------|\n",
    "| Data loading | `train.py` | Reads loan dataset from `data/train.csv`. |\n",
    "| Feature engineering logic | `featureproc.py` | Custom transformer class `FeatureEngineering`. |\n",
    "| Model training & hyperparameter tuning | `train.py` | Uses tuned XGBoost model parameters finalized from notebook experiments. |\n",
    "| Decision threshold selection (`0.45`) | `train.py` | Threshold locked based on best F1 score from validation results. |\n",
    "| Final model training | `train.py` | Trains full XGBoost model on entire training dataset. |\n",
    "| Model serialization (pipeline + threshold) | `train.py` | Saved using `pickle` as `models/loanpayback_pipeline.bin`. |\n",
    "| API-based prediction logic | `predict.py` | Loads trained pipeline and serves predictions via FastAPI. |\n",
    "\n",
    "### Example: Model Saving in `train.py`\n",
    "```python\n",
    "model_path = \"models/loanpayback_pipeline.bin\"\n",
    "\n",
    "with open(model_path, \"wb\") as f_out:\n",
    "    pickle.dump({\"pipeline\": pipeline, \"threshold\": best_threshold}, f_out)\n",
    "\n",
    "\n",
    "### \n",
    "Example: Model Loading in `predict.py`\n",
    "```python\n",
    "with open(MODEL_PATH, \"rb\") as f_in:\n",
    "    model_data = pickle.load(f_in)\n",
    "\n",
    "pipeline = model_data[\"pipeline\"]\n",
    "threshold = model_data[\"threshold\"]\n",
    "```\n",
    "\n",
    "## Reproducibility\n",
    "\n",
    "This project is fully reproducible. The dataset, notebook, and training scripts are included in the repository, allowing seamless re-execution.\n",
    "\n",
    "- Dataset available in `data/train.csv`\n",
    "- Full analysis in `notebooks/LoanPaybackNB.ipynb`\n",
    "- Feature Training function in `src/featureproc.py`\n",
    "- Final model training located in `src/train.py`\n",
    "- Inference logic exposed via `src/predict.py`\n",
    "- Trained pipeline saved at `models/loanpayback_pipeline.bin`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4db45b-d86c-49c6-bd23-c2cf7939538b",
   "metadata": {},
   "source": [
    "### How to Reproduce\n",
    "```bash\n",
    "# Install dependencies and set up environment\n",
    "uv sync\n",
    "\n",
    "# Run training script\n",
    "uv run python -m src.train\n",
    "\n",
    "# Start inference API\n",
    "uv run uvicorn src.predict:app --reload --port 8000\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Model Deployment (Local)\n",
    "\n",
    "The trained machine learning model is deployed locally using **FastAPI** and served via **Uvicorn**.\n",
    "\n",
    "### Start API Locally\n",
    "```bash\n",
    "uv run uvicorn src.predict:app --reload --port 8000\n",
    "```\n",
    "\n",
    "Once the application is running:\n",
    "\n",
    "- **Swagger UI (API documentation):** `http://localhost:8000/docs`\n",
    "- **Root endpoint:** `http://localhost:8000/`\n",
    "\n",
    "### Supported Features\n",
    "- **Single loan prediction API (POST):** `/predict`\n",
    "- **HTML-based UI for interactive prediction:** `/ui`\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b006255-bdf3-4277-9369-841dd5ed1d59",
   "metadata": {},
   "source": [
    "## Dependency & Environment Management\n",
    "\n",
    "The project uses **uv** to manage dependencies and execution. All required packages are defined in `pyproject.toml` and `requirements.txt`.\n",
    "\n",
    "### Install Dependencies\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "### Example Execution Commands\n",
    "```bash\n",
    "uv run python -m src.train      # Train model\n",
    "uv run uvicorn src.predict:app --reload --port 8000   # Launch API\n",
    "```\n",
    "---\n",
    "\n",
    "## Dependency Files\n",
    "\n",
    "### `requirements.txt`\n",
    "```txt\n",
    "fastapi==0.128.0\n",
    "joblib==1.5.3\n",
    "numpy==2.4.0\n",
    "pandas==2.3.3\n",
    "pydantic==2.12.5\n",
    "pydantic_core==2.41.5\n",
    "scikit-learn==1.8.0\n",
    "scipy==1.16.3\n",
    "seaborn==0.13.2\n",
    "uv==0.9.21\n",
    "uvicorn==0.40.0\n",
    "xgboost==3.1.2\n",
    "\n",
    "```\n",
    "\n",
    "### `pyproject.toml`\n",
    "```toml\n",
    "[project]\n",
    "name = \"loanpayback\"\n",
    "version = \"0.1.0\"\n",
    "description = \"Loan Payback Predicion Project\"\n",
    "readme = \"README.md\"\n",
    "requires-python = \">=3.12\"\n",
    "dependencies = [\n",
    "    \"fastapi>=0.128.0\",\n",
    "    \"pandas>=2.3.3\",\n",
    "    \"scikit-learn>=1.8.0\",\n",
    "    \"uvicorn>=0.40.0\",\n",
    "    \"xgboost>=3.1.2\",\n",
    "    \"pydantic>=2.12.5\",\n",
    "    \"pydantic_core>=2.41.5\",\n",
    "    \"numpy>=2.4.0\",\n",
    "    \"pandas>=2.3.3\",\n",
    "    \"scipy==1.16.3\",\n",
    "    \"notebook>=7.5.1\",\n",
    "]\n",
    "\n",
    "[dependency-groups]\n",
    "dev = [\n",
    "    \"requests>=2.32.5\",\n",
    "]\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "## Containerization (Docker)\n",
    "\n",
    "The project is fully containerized using **Docker**, allowing consistent deployment across environments.\n",
    "### Dockerfile Used\n",
    "```dockerfile\n",
    "# Use lightweight Python\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# ==============================\n",
    "# Environment settings\n",
    "# ==============================\n",
    "ENV PYTHONDONTWRITEBYTECODE=1\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "\n",
    "# ==============================\n",
    "# Working directory\n",
    "# ==============================\n",
    "WORKDIR /app\n",
    "\n",
    "# ==============================\n",
    "# System dependencies (XGBoost)\n",
    "# ==============================\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    build-essential \\\n",
    "    libgomp1 \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# ==============================\n",
    "# Python dependencies\n",
    "# ==============================\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# ==============================\n",
    "# Copy only required files\n",
    "# ==============================\n",
    "COPY src ./src\n",
    "COPY models ./models\n",
    "\n",
    "# ==============================\n",
    "# Expose API port\n",
    "# ==============================\n",
    "EXPOSE 8000\n",
    "\n",
    "# ==============================\n",
    "# Run FastAPI\n",
    "# ==============================\n",
    "CMD [\"uvicorn\", \"src.predict:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\n",
    "```\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c06c6-6cdc-4b1a-87cb-b77b503675ae",
   "metadata": {},
   "source": [
    "### Build the Docker Image\n",
    "\n",
    "Run the following command inside the project folder:\n",
    "```bash\n",
    "docker build --no-cache -t loanpayback-api .\n",
    "```\n",
    "---\n",
    "\n",
    "### Run the Docker Container\n",
    "```bash\n",
    "docker run -p 8000:8000 loanpayback-api\n",
    "```\n",
    "\n",
    "Once started, the API will be available at:\n",
    "- **Local URL** → `http://localhost:8000/docs/`\n",
    "- **Swagger UI** → `http://localhost:8000/docs/`\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08d7ba-df22-4bc8-811d-527c47072fe2",
   "metadata": {},
   "source": [
    "## Cloud Deployment\n",
    "\n",
    "The Loan Payback Prediction API is deployed on Render using FastAPI and Docker, enabling real-time loan repayment inference through RESTful endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313963e8-6107-4a82-afad-b6aecbdadd2c",
   "metadata": {},
   "source": [
    "### Deployment Steps (Docker + Render)\n",
    "\n",
    "#### 1. Push complete project to GitHub\n",
    "[github repo link]\n",
    "(https://github.com/codevalhalla/ml-ecommerce-fraud-detection)\n",
    "\n",
    "\n",
    "#### 2. On Render Dashboard → “New Web Service”\n",
    "\n",
    "#### 3. Select Deployment Settings\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| Environment |\tDocker |\n",
    "| Repository | `codevalhalla/ml-ecommerce-fraud-detection` |\n",
    "| Branch | main |\n",
    "| Root Directory | `(leave empty)` |\n",
    "| Environment Variables | `PORT=8000` |\n",
    "| Instance Type | Free Tier |\n",
    "\n",
    "#### 4. Click \"Deploy Web Service\"\n",
    "\n",
    "Render automatically:\n",
    "\n",
    "* Pulls repo\n",
    "\n",
    "* Builds Docker image\n",
    "\n",
    "* Runs FastAPI service using command from Dockerfile\n",
    "```CSS\n",
    "CMD [\"uvicorn\", \"src.predict:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c0566-727c-4b4b-8038-a53eb5dc8ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
